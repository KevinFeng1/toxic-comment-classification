{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Input, Dense, Embedding, SpatialDropout1D, concatenate, GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for null values in train set:\n",
      " id               0\n",
      "comment_text     0\n",
      "toxic            0\n",
      "severe_toxic     0\n",
      "obscene          0\n",
      "threat           0\n",
      "insult           0\n",
      "identity_hate    0\n",
      "dtype: int64\n",
      "Check for null values in test set:\n",
      " id              0\n",
      "comment_text    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Check for null values in train set:\\n', train.isnull().sum())\n",
    "print('Check for null values in test set:\\n', test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total toxic comments:           15294\n",
      "Total severe_toxic comments:    1595\n",
      "Total obscene comments:         8449\n",
      "Total threat comments:          478\n",
      "Total insult comments:          7877\n",
      "Total identity_hate comments:   1405\n"
     ]
    }
   ],
   "source": [
    "print('Total toxic comments:          ',np.sum(train['toxic']))\n",
    "print('Total severe_toxic comments:   ',np.sum(train['severe_toxic']))\n",
    "print('Total obscene comments:        ',np.sum(train['obscene']))\n",
    "print('Total threat comments:         ',np.sum(train['threat']))\n",
    "print('Total insult comments:         ',np.sum(train['insult']))\n",
    "print('Total identity_hate comments:  ',np.sum(train['identity_hate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest comment: 1 words\n",
      "Longest comment:  773 words\n"
     ]
    }
   ],
   "source": [
    "## find length of longest and shortested char. max_len to be used to set input shape\n",
    "\n",
    "max_len = len(max(pd.concat([train['comment_text'], test['comment_text']]), key = len).split())\n",
    "min_len = len(min(pd.concat([train['comment_text'], test['comment_text']]), key = len).split())\n",
    "\n",
    "print('Shortest comment: {} words'.format(min_len))\n",
    "print('Longest comment:  {} words'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ROC-AUC accuracy measure during model training - provided by Kaggle\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define functions to create lists with all unique characters and words \n",
    "\n",
    "def unique_char_count(text):\n",
    "    '''returns char_list containing all unique characters in text'''\n",
    "    \n",
    "    character_list = []\n",
    "    index = 0 \n",
    "    for comment in text:\n",
    "        unique_characters = set(list(comment))\n",
    "        index += 1\n",
    "        if index % 50000 == 0:\n",
    "            print('{} rows parsed...'.format(index))\n",
    "        for char in unique_characters:\n",
    "            if char not in character_list:\n",
    "                character_list.append(char)\n",
    "        \n",
    "    return character_list\n",
    "\n",
    "def unique_word_count(text):\n",
    "    '''returns word_list containing all uniques words in text'''\n",
    "    \n",
    "    word_list = []\n",
    "    index = 0 \n",
    "\n",
    "    for comment in text:\n",
    "        unique_words = set(comment.split())\n",
    "        index += 1\n",
    "        if index % 50000 == 0:\n",
    "            print('{} rows parsed...'.format(index))\n",
    "        for word in unique_words:\n",
    "            word_list.append(word)\n",
    "\n",
    "    word_list = set(word_list)\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rows parsed...\n",
      "100000 rows parsed...\n",
      "150000 rows parsed...\n",
      "50000 rows parsed...\n",
      "100000 rows parsed...\n",
      "150000 rows parsed...\n",
      "There are 2335 unique characters in the train set.\n",
      "There are 5112 unique characters in the test set.\n",
      "3207 characters in test set that are not in train set.\n"
     ]
    }
   ],
   "source": [
    "# find number of unique characters in both train and test set and identify how many new characters are in test set\n",
    "\n",
    "train_char = unique_char_count(train['comment_text'])\n",
    "test_char = unique_char_count(test['comment_text'])\n",
    "\n",
    "differences = []\n",
    "for i in test_char:\n",
    "    if i not in train_char:\n",
    "        differences.append(i)\n",
    "\n",
    "print('There are {} unique characters in the train set.'.format(len(train_char)))\n",
    "print('There are {} unique characters in the test set.'.format(len(test_char)))\n",
    "print('{} characters in test set that are not in train set.'.format(len(differences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rows parsed...\n",
      "100000 rows parsed...\n",
      "150000 rows parsed...\n",
      "50000 rows parsed...\n",
      "100000 rows parsed...\n",
      "150000 rows parsed...\n",
      "There are 532299 unique words in the train set.\n",
      "There are 611496 unique words in the test set.\n",
      "429930 words in test set that are not in train set.\n"
     ]
    }
   ],
   "source": [
    "# find the number of unique words in both train and test set and identify how many new characters are in test set\n",
    "\n",
    "train_words = unique_word_count(train['comment_text'])\n",
    "test_words = unique_word_count(test['comment_text'])\n",
    "\n",
    "differences = []\n",
    "for i in test_words:\n",
    "    if i not in train_words:\n",
    "        differences.append(i)\n",
    "\n",
    "print('There are {} unique words in the train set.'.format(len(train_words)))\n",
    "print('There are {} unique words in the test set.'.format(len(test_words)))\n",
    "print('{} words in test set that are not in train set.'.format(len(differences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_embed_vecs(file):\n",
    "    '''read pre-trained embedding file and return:\n",
    "    word_to_vec_map mapping words to embedding vector\n",
    "    words_to_index mapping words to respective index\n",
    "    index_to_words mapping index to respective word\n",
    "    \n",
    "    note: FastText files is utf8 encoded'''\n",
    "    \n",
    "    with open(file, 'r', encoding=\"utf8\") as f:\n",
    "        \n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            current_word = line[0]\n",
    "            words.add(current_word)\n",
    "            word_to_vec_map[current_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        \n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "            \n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pre-trained word vectors from:  \n",
    "@inproceedings{mikolov2018advances,\n",
    "  title={Advances in Pre-Training Distributed Word Representations},\n",
    "  author={Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},\n",
    "  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},\n",
    "  year={2018}\n",
    "}'''\n",
    "\n",
    "words_to_index, index_to_words, word_to_vec_map = read_embed_vecs('D:\\\\FastText\\\\crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split to train and test sets\n",
    "X_train = train['comment_text']\n",
    "Y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "X_test = test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use 5% of total words \n",
    "max_words = int((len(train_words) + len(test_words)) * .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize words using 10% of total words\n",
    "tokenizer = text.Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest comment in train set is: 1401 tokens\n",
      "Longest comment in test set is:  2142 tokens\n"
     ]
    }
   ],
   "source": [
    "print('Longest comment in train set is: {} tokens'.format(len(max(X_train, key = len))))\n",
    "print('Longest comment in test set is:  {} tokens'.format(len(max(X_test, key = len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad sequences for uniform length. allow max length of input vector to be 1/3 of longest comment\n",
    "\n",
    "max_len = int(max_len/3)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize embedding matrix with 0's or pre-trained feature vector where available\n",
    "# max words + 1 for keras embedding\n",
    "\n",
    "embed_size = 300\n",
    "embed_matrix = np.zeros((max_words, embed_size))\n",
    "\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if idx >= max_words: continue\n",
    "    if word in word_to_vec_map:\n",
    "        embed_matrix[idx, :] = word_to_vec_map[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gru_model():\n",
    "    '''initialize a bidirectional GRU with:\n",
    "    1D spatial dropout\n",
    "    max pooling'''\n",
    "    inp = Input(shape = (max_len, ))\n",
    "    x = Embedding(input_dim = max_words, output_dim = embed_size, weights = [embed_matrix])(inp)\n",
    "    x = SpatialDropout1D(.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences = True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation = 'sigmoid')(conc)    \n",
    "    \n",
    "    model = Model(inputs = inp, outputs = outp)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gru_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\envs\\data-x\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "x_tra, x_val, y_tra, y_val = train_test_split(X_train, Y_train, train_size = .95, random_state = 1)\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_val, y_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_tra, y_tra, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val), callbacks=[RocAuc],\\\n",
    "                 verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size = 1024)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('submission_kf_gru_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
